# LanguageLeapAI

![](docs/screenshots/LanguageLeapAI_logo.png?raw=true)

LEAP across Language barriers by using AI to converse with other online users from across the globe!
**LanguageLeapAI** aims to provide you a real-time language AI assistant that can understand and speak your desired language fluently. 
*(Targeted towards English to Japanese as of right now)*

Setup Guide: https://www.youtube.com/watch?v=bN5UaEkIPGM

Showcase: https://www.youtube.com/watch?v=UY7sRB60wZ4


## Integration of AI Entities

This project integrates 3 free and open-source AI systems:

1. [WhisperAI](https://github.com/openai/whisper): General-purpose Speech Recognition Model developed by OpenAI that can perform multilingual speech
recognition.
2. [DeepL Translator](https://www.deepl.com/translator): Powered by neural networks and the latest AI innovations for natural-sounding translations
3. [Voicevox](https://voicevox.hiroshiba.jp/): Japanese Deep-Learning AI Voice Synthesizer


WhisperAI and Voicevox both have docker images available on DockerHub, so we will be building and running them both via a [Docker Compose file](docker-compose.yml).
DeepL can be interacted with by signing up for a free plan and interacting with its [REST API](https://www.deepl.com/pro-api?cta=header-pro-api/) up to 500,000 character limit / month

![](docs/screenshots/ai_integrations.png?raw=true)

## How it works

**LanguageLeapAI** is made up of 2 main python programs.


### Voice Translator

The first, [voice_translator.py](src/voice_translator.py), records your microphone whenever a push-to-talk key is held down on the keyboard.
Once this key is released, it saves your voice in an audio file which is then sent to WhisperAI's transcribe endpoint which runs Automatic Speech Recognition (ASR) on it.
After a response containing your speech as text is received, this text is then translated using DeepL's REST API. 

The translated text is then sent to Voicevox which performs text-to-speech and generates an audio file voiced in Japanese.
This file is then played to your target application's microphone input and your speakers/headphones.

Since Voicevox only takes in Japanese text as input and generates speech in Japanese, the project is technically only limited to Japanese as the target language.
However, Voicevox can be replaced with any other text to speech program that can speak your desired language for limitless possibilities.


### Audio Subtitler

The second, [subtitler.py](src/subtitler.py), records your application's audio output and listens in the background for any speech.
Once it has detected that a phrase/sentence is complete, it saves the audio into a wav file and sends it to WhisperAI's translate endpoint which translates the speech from the target language to English.

This English text is then displayed on screen using python's tkinter module, essentially acting as subtitles.


## Applications

**LanguageLeapAI's** target audience is for users who want to chat with another but do not speak the same language.
An example is an English-speaking user playing an online game in the Japan server but wants to use voice chat despite not knowing Japanese.

By running both [subtitler.py](src/subtitler.py) and [voice_translator.py](src/voice_translator.py), they can understand their fellow Japanese teammates by reading the english subtitles generated in real time.
They can also speak English and the Japanese teammates will instead hear the translated Japanese speech generated by Voicevox.

However,  this is not the only application of **LanguageLeapAI**.

### Only using Audio Subtitler

User simply wants to understand what is being said with no need to speak. E.g. Watching a video / stream / movie in another language without subtitles.
The user can choose to not run [voice_translator.py](src/voice_translator.py) and simply use [subtitler.py](src/subtitler.py).

### Only using Voice Translator

User understands the language enough to listen and understand, but is afraid to speak the language for various reasons, e.g. Anonymity / Fear of messing up or offending.
The user can choose to not run [subtitler.py](src/subtitler.py) and simply use [voice_translator.py](src/voice_translator.py).


## Setup

Setting up **LanguageLeapAI** requires 3 crucial steps, so don't miss out on any of them!

[//]: # (1. [Installing Services and Dependencies]&#40;docs/INSTALLATION.md&#41;)
[//]: # (2. [Audio Routing]&#40;docs/AUDIO.md&#41;)

[//]: # (3. [Writing your Environment file]&#40;docs/ENV.md&#41;)

1. Install [python](https://www.python.org/downloads/) and make sure to click "add python to PATH" in the installer

2. Install python requirements

```shell
python install -r requirements.txt
```

3. Install Voicevox requirements

- [voicevox](https://github.com/VOICEVOX/voicevox_core/releases)
  - copy the right .whl link
  - pip install <link> 
  - example: `pip install https://github.com/VOICEVOX/voicevox_core/releases/download/0.14.2/voicevox_core-0.14.2+cuda-cp38-abi3-win_amd64.whl`

- [onnxruntime](https://github.com/microsoft/onnxruntime/releases)
  - get the right version for your os from github releases ex: https://github.com/microsoft/onnxruntime/releases/download/v1.14.1/onnxruntime-win-x64-gpu-1.14.1.zip
  - copy onnxruntime.dll and onnruntime_providers_shared.dll in the modules/ folder
  - macOS:
    - move the extracted folder to /usr/local/lib
    - `sudo ln -s /usr/local/lib/onnxruntime-osx-arm64-1.13.1/lib/libonnxruntime.1.13.1.dylib /usr/local/lib/libonnxruntime.1.13.1.dylib`

- [open_jtalk_dict](https://sourceforge.net/projects/open-jtalk/files/Open%20JTalk/open_jtalk-1.11/open_jtalk-1.11.tar.gz/download)
  - put the extracted folder path in OPEN_JTALK_DICT_DIR in .env
  - macOS: you can put the folder in /usr/local/<extracted_openjtalk_folder>


## Usage

To run **LanguageLeapAI**, you need to first run WhisperAI and Voicevox. They can either be run via Docker or using Google Colab.

### Python Program

Run these commands in the [src/](src) folder.

To run the Audio Subtitler:

```python subtitler.py```

To run the Voice Translator:

```python voice_translator.py```

To stop the python scripts, simply press `Ctrl+C` in the terminal.


### Things to note

Some important things to keep in mind while using **LanguageLeapAI**.


#### Whisper's inconsistency

Do note that WhisperAI is not exactly the most accurate and will not transcribe speech correctly 100% of the time, so use at your own risk.
Until OpenAI decides to improve the dataset that was used to train the Whisper models, this will have to do.

Also, Whisper is not designed to handle multiple concurrent requests at once.
However, for subtitles to be updated in time, multiple requests are being sent asynchronously, so some requests might return an error.


#### Voicevox voices

There are certain terms and conditions for using the voices from Voicevox, so do read up on [these](https://voicevox.hiroshiba.jp/) before using a specific speaker.


#### Application limitations

Some applications like Valorant for some reason does not allow open mic for team voice chat, so **LanguageLeapAI** will not work for in these cases,
unless you hold down the push to talk button whenever you want your teammates to hear the Text-to-Speech.
However, Valorant does have open mic for party voice-chat, so there should be no issue if it's used towards your party members.


## License

The code of LanguageLeapAI is released under the MIT License. See [LICENSE](LICENSE) for further details.
